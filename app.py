# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZoVm35bbCaV7779s2Dsql8v4xLdlMZfC
"""

import streamlit as st
import torch
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration
import requests
from bs4 import BeautifulSoup

# 1. í˜ì´ì§€ ê¸°ë³¸ ì„¤ì • (íƒ­ ì´ë¦„, ì•„ì´ì½˜ ë“±)
st.set_page_config(page_title="ì™¸êµë¶€ ì†Œì‹ ìš”ì•½ ë´‡", page_icon="ğŸ¤–")

# 2. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸° (ìºì‹±ì„ ì‚¬ìš©í•˜ì—¬ ì†ë„ í–¥ìƒ)
# ì£¼ì˜: 'final_model' í´ë”ê°€ app.pyì™€ ê°™ì€ ìœ„ì¹˜ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
@st.cache_resource
def load_model():
    try:
        model_path = "./final_model"  # í´ë” ì´ë¦„
        tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)
        model = BartForConditionalGeneration.from_pretrained(model_path)
        return tokenizer, model
    except Exception as e:
        st.error(f"ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. í´ë” ìœ„ì¹˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”! ì—ëŸ¬ ë‚´ìš©: {e}")
        return None, None

tokenizer, model = load_model()

# 3. í…ìŠ¤íŠ¸ ìš”ì•½ í•¨ìˆ˜
def summarize_text(text):
    if not text:
        return "ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."

    # ëª¨ë¸ ì…ë ¥ ë°ì´í„°ë¡œ ë³€í™˜
    input_ids = tokenizer.encode(text, return_tensors="pt")

    # ëª¨ë¸ì´ ìš”ì•½ë¬¸ ìƒì„±
    summary_text_ids = model.generate(
        input_ids=input_ids,
        bos_token_id=model.config.bos_token_id,
        eos_token_id=model.config.eos_token_id,
        length_penalty=2.0,
        max_length=128,
        min_length=32,
        num_beams=4,
    )

    # ìˆ«ìë¡œ ëœ ê²°ê³¼ë¥¼ ë‹¤ì‹œ ê¸€ìë¡œ ë³€í™˜
    return tokenizer.decode(summary_text_ids[0], skip_special_tokens=True)

# 4. ì›¹ì‚¬ì´íŠ¸ ë³¸ë¬¸ ê¸ì–´ì˜¤ê¸° (í¬ë¡¤ë§) í•¨ìˆ˜
def scrape_text(url):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')

        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ì¸ ê²½ìš° (iframe ì²˜ë¦¬)
        if "blog.naver.com" in url:
            iframe = soup.select_one("iframe#mainFrame")
            if iframe:
                blog_url = "https://blog.naver.com" + iframe["src"]
                response = requests.get(blog_url, headers=headers)
                soup = BeautifulSoup(response.text, 'html.parser')

        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (p íƒœê·¸ë‚˜ div íƒœê·¸ ìœ„ì£¼)
        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë³¸ë¬¸ í´ë˜ìŠ¤ ë“±ì„ íƒ€ê²ŸíŒ…
        content = ""
        main_container = soup.select_one(".se-main-container") or soup.select_one(".post-view")

        if main_container:
            content = main_container.get_text("\n")
        else:
            # ì¼ë°˜ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ë“±
            paragraphs = soup.find_all('p')
            content = " ".join([p.get_text() for p in paragraphs])

        return content.strip()[:2000] # ë„ˆë¬´ ê¸¸ë©´ ëª¨ë¸ì´ í˜ë“¤ì–´í•˜ë¯€ë¡œ ì•ë¶€ë¶„ 2000ìë§Œ ì‚¬ìš©

    except Exception as e:
        return f"ì—ëŸ¬ ë°œìƒ: {e}"

# 5. ì™¸êµë¶€ ë¸”ë¡œê·¸ ìµœì‹  ê¸€ ê°€ì ¸ì˜¤ê¸° (RSS ëŒ€ìš© ê°„ë‹¨ í¬ë¡¤ë§)
def get_mofa_news():
    url = "https://blog.naver.com/PostList.naver?blogId=mofakr&categoryNo=0" # ì™¸êµë¶€ ë¸”ë¡œê·¸ ì „ì²´ê¸€
    headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')

    # ìµœì‹  ê¸€ ì œëª©ê³¼ ë§í¬ 3ê°œë§Œ ê°€ì ¸ì˜¤ê¸° (ì˜ˆì‹œ êµ¬ì¡°)
    news_list = []
    titles = soup.select("span.ell") # ë„¤ì´ë²„ ëª¨ë°”ì¼/ë¦¬ìŠ¤íŠ¸ ë·° í´ë˜ìŠ¤ëª… ì¶”ì •

    # ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•˜ê¸° ìœ„í•´ í•˜ë“œì½”ë”©ëœ ì˜ˆì‹œ ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ê±°ë‚˜
    # ì‹¤ì œë¡œëŠ” ë„¤ì´ë²„ ë¸”ë¡œê·¸ êµ¬ì¡°ì— ë§ì¶° ì •í™•í•œ íŒŒì‹±ì´ í•„ìš”í•©ë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” 'URL ìš”ì•½' ê¸°ëŠ¥ì´ ë©”ì¸ì´ë¯€ë¡œ, ì™¸êµë¶€ ê³µì‹ ë¸”ë¡œê·¸ ë§í¬ë¥¼ ì•ˆë‚´í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.

    return [
        {"title": "ì™¸êµë¶€ ê³µì‹ ë¸”ë¡œê·¸ ë°”ë¡œê°€ê¸° (í´ë¦­í•´ì„œ URLì„ ë³µì‚¬í•´ë³´ì„¸ìš”)", "link": "https://blog.naver.com/mofakr"},
        {"title": "[ì˜ˆì‹œ] ìµœê·¼ ì™¸êµë¶€ ë³´ë„ìë£Œ", "link": "https://www.mofa.go.kr/www/brd/m_4080/list.do"}
    ]


# --- ë©”ì¸ í™”ë©´ êµ¬ì„± (UI) ---

st.title("ğŸ“° ì™¸êµë¶€ ì†Œì‹ ìë™ ìš”ì•½ ë´‡")
st.write("ì¸ê³µì§€ëŠ¥ì´ ë³µì¡í•œ ë‰´ìŠ¤ë¥¼ 3ì¤„ë¡œ ìš”ì•½í•´ ë“œë¦½ë‹ˆë‹¤.")

if model is None:
    st.error("âš ï¸ 'final_model' í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. ê°™ì€ í´ë”ì— ëª¨ë¸ì„ ë„£ì–´ì£¼ì„¸ìš”!")
else:
    st.success("AI ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ! (Ready)")

# íƒ­ ë§Œë“¤ê¸°
tab1, tab2 = st.tabs(["ğŸ”— URL ìš”ì•½í•˜ê¸°", "ğŸ“¢ ì™¸êµë¶€ ì†Œì‹í†µ"])

# [Tab 1] URL ì…ë ¥í•´ì„œ ìš”ì•½í•˜ê¸°
with tab1:
    st.header("ë‰´ìŠ¤/ë¸”ë¡œê·¸ ì£¼ì†Œ ì…ë ¥")
    input_url = st.text_input("ìš”ì•½í•˜ê³  ì‹¶ì€ ë„¤ì´ë²„ ë¸”ë¡œê·¸ë‚˜ ë‰´ìŠ¤ ê¸°ì‚¬ URLì„ ì…ë ¥í•˜ì„¸ìš”:")

    if st.button("ìš”ì•½ ì‹œì‘"):
        if input_url:
            with st.spinner('ë‰´ìŠ¤ ë‚´ìš©ì„ ì½ì–´ì˜¤ê³  ìš”ì•½í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...'):
                # 1. í¬ë¡¤ë§
                raw_text = scrape_text(input_url)

                if "ì—ëŸ¬ ë°œìƒ" in raw_text:
                    st.error(raw_text)
                elif len(raw_text) < 50:
                    st.warning("ë‚´ìš©ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì ‘ê·¼ì´ ë§‰í˜€ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                else:
                    # 2. ìš”ì•½
                    summary = summarize_text(raw_text)

                    st.subheader("ğŸ“ ìš”ì•½ ê²°ê³¼")
                    st.info(summary)

                    with st.expander("ì›ë³¸ ë‚´ìš© ë³´ê¸°"):
                        st.write(raw_text)
        else:
            st.warning("URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")

# [Tab 2] ì™¸êµë¶€ ì†Œì‹ ê°€ì ¸ì˜¤ê¸°
with tab2:
    st.header("ì™¸êµë¶€ ìµœì‹  ì†Œì‹")
    st.write("ì•„ë˜ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ì™¸êµë¶€ ê´€ë ¨ ë§í¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.")

    if st.button("ìµœì‹  ì†Œì‹ ê°€ì ¸ì˜¤ê¸°"):
        news_items = get_mofa_news()

        for item in news_items:
            st.markdown(f"**ğŸ“Œ {item['title']}**")
            st.write(f"ë§í¬: {item['link']}")
            st.write("---")

        st.info("ğŸ’¡ ìœ„ ë§í¬ ì¤‘ í•˜ë‚˜ë¥¼ ë³µì‚¬í•´ì„œ https://blog.naver.com/gjfls01/221792823369?viewType=pc íƒ­ì— ë„£ì–´ë³´ì„¸ìš”!")