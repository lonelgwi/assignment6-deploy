import streamlit as st
import torch
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration
import requests
from bs4 import BeautifulSoup
import re
import trafilatura  # [í•µì‹¬] ê°•ë ¥í•œ í¬ë¡¤ë§ ë„êµ¬

# ==========================================
# 1. í˜ì´ì§€ ë° ëª¨ë¸ ì„¤ì •
# ==========================================
st.set_page_config(page_title="ì™¸êµë¶€ ì†Œì‹ ìš”ì•½ ë´‡", page_icon="ğŸ¤–")

@st.cache_resource
def load_model():
    try:
        # ê¹ƒí—ˆë¸Œ ìš©ëŸ‰/LFS ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ Hugging Face Hubì˜ ê³µê°œ ëª¨ë¸ ì‚¬ìš©
        model_name = "gogamza/kobart-summarization" 
        tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)
        model = BartForConditionalGeneration.from_pretrained(model_name)
        return tokenizer, model
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None, None

tokenizer, model = load_model()

# ==========================================
# 2. [ìµœì¢… í•´ê²°ì±…] Trafilatura í¬ë¡¤ë§ í•¨ìˆ˜
# ==========================================
def get_naver_blog_content(url):
    """
    ì¼ë°˜ requests ëŒ€ì‹  trafilatura ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë„¤ì´ë²„ ë¸”ë¡œê·¸ì˜ ë³µì¡í•œ êµ¬ì¡°ì™€ ë´‡ ì°¨ë‹¨ì„
    ë” íš¨ê³¼ì ìœ¼ë¡œ ìš°íšŒí•˜ì—¬ ë³¸ë¬¸ë§Œ ì¶”ì¶œí•´ëƒ…ë‹ˆë‹¤.
    """
    if not url:
        return "ì—ëŸ¬", "URLì´ ì—†ìŠµë‹ˆë‹¤."

    try:
        # 1. ëª¨ë°”ì¼ ì£¼ì†Œ ë³€í™˜ (ëª¨ë°”ì¼ í˜ì´ì§€ê°€ í¬ë¡¤ë§ ì„±ê³µë¥ ì´ ë†’ìŒ)
        if "m.blog.naver.com" in url:
            target_url = url.replace("m.blog.naver.com", "blog.naver.com")
        else:
            target_url = url

        # 2. trafilaturaë¡œ ë‹¤ìš´ë¡œë“œ ì‹œë„ (ë„¤ì´ë²„ ì°¨ë‹¨ ìš°íšŒ ì‹œë„)
        downloaded = trafilatura.fetch_url(target_url)
        
        # 3. ì‹¤íŒ¨ ì‹œ, PostView ì „ìš© ì£¼ì†Œë¡œ ì¬ì‹œë„ (2ì°¨ ì‹œë„)
        if downloaded is None:
            match = re.search(r'blog\.naver\.com/([a-zA-Z0-9_]+)/([0-9]+)', target_url)
            if match:
                blog_id = match.group(1)
                log_no = match.group(2)
                # Iframe ì—†ëŠ” ìˆœìˆ˜ ë³¸ë¬¸ URL
                final_url = f"https://blog.naver.com/PostView.naver?blogId={blog_id}&logNo={log_no}"
                downloaded = trafilatura.fetch_url(final_url)

        # 4. ê·¸ë˜ë„ ì—†ìœ¼ë©´ ì‹¤íŒ¨ ì²˜ë¦¬
        if downloaded is None:
            return "ì ‘ì† ì‹¤íŒ¨", None

        # 5. ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        result_text = trafilatura.extract(downloaded, include_comments=False, include_tables=False, include_links=False)
        
        # ì œëª© ì¶”ì¶œ (ë©”íƒ€ íƒœê·¸ í™œìš©)
        soup = BeautifulSoup(downloaded, 'html.parser')
        og_title = soup.select_one('meta[property="og:title"]')
        title = og_title['content'] if og_title else "ì œëª© ì—†ìŒ"

        if result_text:
            # ì¤„ë°”ê¿ˆ ì •ë¦¬
            text = re.sub(r'\n+', ' ', result_text)
            return title, text.strip()
        else:
            return title, None

    except Exception as e:
        return "ì—ëŸ¬", f"í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì—ëŸ¬: {e}"

# ==========================================
# 3. RSS íŒŒì‹± í•¨ìˆ˜ (ì œëª© ê¹¨ì§ í•´ê²° + í•„í„°ë§)
# ==========================================
def clean_html_title(raw_html):
    """ì œëª©ì— ë¶™ì€ ì§€ì €ë¶„í•œ íƒœê·¸(CDATA ë“±) ì œê±°"""
    if not raw_html: return ""
    clean = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', raw_html) # CDATA ì œê±°
    clean = re.sub(r'<.*?>', '', clean) # HTML íƒœê·¸ ì œê±°
    clean = re.sub(r'&[a-z]+;', '', clean) # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    return clean.strip()

def get_latest_mofa_news():
    rss_url = "https://rss.blog.naver.com/mofakr.xml"
    
    # ì¼ë°˜ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ê²Œ í•˜ëŠ” í—¤ë”
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        response = requests.get(rss_url, headers=headers, timeout=5)
        # lxml ì—†ì–´ë„ ë˜ë„ë¡ html.parser ì‚¬ìš©
        soup = BeautifulSoup(response.content, 'html.parser') 
        
        items = soup.find_all('item')
        target_links = []
        
        for item in items:
            # RSS íƒœê·¸ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
            category = item.category.text if item.category else ""
            raw_title = item.title.text if item.title else ""
            link = item.link.text.strip() if item.link else ""
            
            # ì œëª© ì •ì œ
            title = clean_html_title(raw_title)

            if not link: continue

            # [í•„í„°ë§] 'ì†Œì‹', 'ë³´ë„', 'ëŒ€ë³€ì¸' í‚¤ì›Œë“œê°€ ë“¤ì–´ê°„ ê¸€ë§Œ ìˆ˜ì§‘
            if "ì†Œì‹" in category or "ë³´ë„" in category or "ëŒ€ë³€ì¸" in category or "ì™¸êµë¶€" in category:
                target_links.append({"title": title, "link": link})
                if len(target_links) >= 5: # 5ê°œ ëª¨ìœ¼ë©´ ë
                    break
        
        # í•„í„°ë§ ëœ ê²Œ ì—†ìœ¼ë©´ ìµœì‹ ê¸€ 3ê°œë¼ë„ ê°€ì ¸ì˜´ (ë¹„ìƒìš©)
        if not target_links and items:
             for i in items[:3]:
                t = clean_html_title(i.title.text)
                l = i.link.text.strip()
                if l: # ë§í¬ê°€ ìˆì„ ë•Œë§Œ
                    target_links.append({"title": t, "link": l})

        return target_links

    except Exception as e:
        print(f"RSS ì—ëŸ¬: {e}")
        return []

# ==========================================
# 4. ìš”ì•½ í•¨ìˆ˜ (KoBART)
# ==========================================
def predict_summary(text):
    if not text or len(text) < 50:
        return "ìš”ì•½í•  ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    # ì…ë ¥ ë°ì´í„° ë³€í™˜
    input_ids = tokenizer.encode(text, return_tensors="pt", max_length=1024, truncation=True)

    # ìš”ì•½ë¬¸ ìƒì„±
    summary_text_ids = model.generate(
        input_ids=input_ids,
        bos_token_id=model.config.bos_token_id,
        eos_token_id=model.config.eos_token_id,
        length_penalty=1.2,
        max_length=256,
        min_length=30,
        num_beams=4,
        early_stopping=True,
        no_repeat_ngram_size=3
    )
    
    summary = tokenizer.decode(summary_text_ids[0], skip_special_tokens=True)

    # ë¬¸ì¥ë¶€í˜¸ ì •ë¦¬
    if summary and summary[-1] not in ['.', '!', '?']:
        last_punctuation = max(summary.rfind('.'), summary.rfind('!'), summary.rfind('?'))
        if last_punctuation != -1:
            summary = summary[:last_punctuation+1]

    return summary

# ==========================================
# 5. ë©”ì¸ UI
# ==========================================
st.title("ğŸ“° ì™¸êµë¶€ ì†Œì‹ ìë™ ìš”ì•½ ë´‡")
st.write("ì¸ê³µì§€ëŠ¥ì´ ì™¸êµë¶€ ë¸”ë¡œê·¸ì˜ ì£¼ìš” ì†Œì‹ì„ 3ì¤„ë¡œ ìš”ì•½í•´ ë“œë¦½ë‹ˆë‹¤.")

if model is None:
    st.error("âš ï¸ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
else:
    st.success("AI ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ! (Ready)")

tab1, tab2 = st.tabs(["ğŸ”— URL ì§ì ‘ ì…ë ¥", "ğŸ“¢ ì™¸êµë¶€ ìµœì‹  ì†Œì‹ (ìë™)"])

# [Tab 1]
with tab1:
    st.subheader("ë‰´ìŠ¤/ë¸”ë¡œê·¸ ì£¼ì†Œ ì…ë ¥")
    input_url = st.text_input("ìš”ì•½í•˜ê³  ì‹¶ì€ ë„¤ì´ë²„ ë¸”ë¡œê·¸ URLì„ ì…ë ¥í•˜ì„¸ìš”:")
    
    if st.button("ìš”ì•½ ì‹œì‘", key="btn1"):
        if input_url:
            with st.spinner('ë¶„ì„ ì¤‘...'):
                title, raw_text = get_naver_blog_content(input_url)
                
                if raw_text:
                    summary = predict_summary(raw_text)
                    st.markdown(f"### ğŸ“„ {title}")
                    st.info(summary)
                    with st.expander("ì›ë³¸ ë‚´ìš© ë³´ê¸°"):
                        st.write(raw_text)
                else:
                    st.error("ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì ‘ê·¼ ê¶Œí•œ ë˜ëŠ” ì‚­ì œëœ ê¸€)")

# [Tab 2]
with tab2:
    st.subheader("ì™¸êµë¶€ ì£¼ìš” ì†Œì‹ (Top 5)")
    st.write("ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ìµœì‹  ì†Œì‹ì„ ê°€ì ¸ì™€ ìš”ì•½í•©ë‹ˆë‹¤.")
    
    if st.button("ìµœì‹  ì†Œì‹ ê°€ì ¸ì˜¤ê¸°", key="btn2"):
        with st.spinner('ì™¸êµë¶€ ë¸”ë¡œê·¸ ìŠ¤ìº” ì¤‘... (ì•½ 10ì´ˆ ì†Œìš”)'):
            news_items = get_latest_mofa_news()
            
            if not news_items:
                st.warning("ìµœì‹  ì†Œì‹ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            else:
                st.success(f"ì´ {len(news_items)}ê°œì˜ ì†Œì‹ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
                
                for i, item in enumerate(news_items):
                    st.markdown("---")
                    st.markdown(f"**[{i+1}] {item['title']}**")
                    
                    # í¬ë¡¤ë§ ì‹œë„
                    _, content = get_naver_blog_content(item['link'])
                    
                    if content:
                        summary = predict_summary(content)
                        st.success(summary)
                    else:
                        # 2ì¤‘ 3ì¤‘ìœ¼ë¡œ ëš«ìœ¼ë ¤ ì‹œë„í–ˆì§€ë§Œ, ê·¸ë˜ë„ ë„¤ì´ë²„ê°€ í´ë¼ìš°ë“œ IPë¥¼ ì›ì²œ ì°¨ë‹¨í•œ ê²½ìš°
                        st.warning("ğŸ”’ ë„¤ì´ë²„ ë³´ì•ˆ ì •ì±…ìœ¼ë¡œ ì¸í•´ ë³¸ë¬¸ ìš”ì•½ì„ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        st.write(f"ğŸ‘‰ [ì›ë¬¸ ë³´ëŸ¬ê°€ê¸°]({item['link']})")
