import streamlit as st
import torch
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration
import requests
from bs4 import BeautifulSoup
import re

# ==========================================
# 1. í˜ì´ì§€ ë° ëª¨ë¸ ì„¤ì •
# ==========================================
st.set_page_config(page_title="ì™¸êµë¶€ ì†Œì‹ ìš”ì•½ ë´‡", page_icon="ğŸ¤–")

@st.cache_resource
def load_model():
    try:
        # ê¹ƒí—ˆë¸Œ ìš©ëŸ‰ ì œí•œ ì—†ì´ ì‹¤í–‰ë˜ë„ë¡ ê³µê°œëœ KoBART ëª¨ë¸ ì‚¬ìš©
        model_name = "gogamza/kobart-summarization" 
        tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)
        model = BartForConditionalGeneration.from_pretrained(model_name)
        return tokenizer, model
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None, None

tokenizer, model = load_model()

# ==========================================
# 2. [í•µì‹¬ ìˆ˜ì •] ê°•ë ¥í•´ì§„ í¬ë¡¤ë§ í•¨ìˆ˜ (PostView ë°©ì‹)
# ==========================================
def get_naver_blog_content(url):
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ URLì—ì„œ blogIdì™€ logNoë¥¼ ì¶”ì¶œí•˜ì—¬
    'PostView.naver' (ë³¸ë¬¸ ì „ìš© URL)ë¡œ ì§ì ‘ ì ‘ì†í•˜ëŠ” ë°©ì‹.
    Streamlit Cloudì—ì„œì˜ ì°¨ë‹¨ì„ ìš°íšŒí•˜ê¸° ìœ„í•¨.
    """
    if not url:
        return "ì—ëŸ¬", "URL ì£¼ì†Œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."

    # 1. í—¤ë” ê°•í™” (ë´‡ ì°¨ë‹¨ íšŒí”¼ìš©)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://www.naver.com/',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8'
    }

    try:
        # 2. URLì—ì„œ blogIdì™€ logNo ì¶”ì¶œ (ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©)
        # ì˜ˆ: https://blog.naver.com/mofakr/224099029110
        # blogId = mofakr, logNo = 224099029110
        
        # ëª¨ë°”ì¼ ì£¼ì†Œë©´ PC ì£¼ì†Œë¡œ 1ì°¨ ë³€í™˜
        if "m.blog.naver.com" in url:
            url = url.replace("m.blog.naver.com", "blog.naver.com")

        # ì •ê·œì‹ìœ¼ë¡œ ì•„ì´ë””ì™€ ê¸€ë²ˆí˜¸ ì°¾ê¸°
        match = re.search(r'blog\.naver\.com/([a-zA-Z0-9_]+)/([0-9]+)', url)
        
        final_url = url # ê¸°ë³¸ì€ ì›ë˜ URL
        
        if match:
            blog_id = match.group(1)
            log_no = match.group(2)
            # iframe ì—†ì´ ë³¸ë¬¸ë§Œ ìˆëŠ” ì „ìš© URL ìƒì„±
            final_url = f"https://blog.naver.com/PostView.naver?blogId={blog_id}&logNo={log_no}&redirect=Dlog&widgetTypeCall=true&directAccess=false"
        
        # 3. ìš”ì²­ ë³´ë‚´ê¸°
        response = requests.get(final_url, headers=headers)
        
        if response.status_code != 200:
            return "ì ‘ì† ì‹¤íŒ¨", f"ì„œë²„ ì‘ë‹µ ì½”ë“œ: {response.status_code}"

        soup = BeautifulSoup(response.text, 'html.parser')

        # 4. ì œëª© ì¶”ì¶œ
        # PostView ë°©ì‹ì—ì„œëŠ” ì œëª© íƒœê·¸ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
        title_elem = soup.select_one('.se-title-text') or soup.select_one('.htitle') or soup.select_one('h3.se_textarea')
        title = title_elem.text.strip() if title_elem else "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"

        # 5. ë³¸ë¬¸ ì¶”ì¶œ
        # PostView ë°©ì‹ì€ #mainFrame(iframe)ì„ ì°¾ì„ í•„ìš”ê°€ ì—†ìŒ. ë°”ë¡œ ë³¸ë¬¸ í´ë˜ìŠ¤ ê²€ìƒ‰.
        content_elem = soup.select_one('.se-main-container') or soup.select_one('#postViewArea') or soup.select_one('.post_view')

        if content_elem:
            text = content_elem.text
            text = re.sub(r'\n+', ' ', text)
            text = re.sub(r'\s+', ' ', text)
            return title, text.strip()
        else:
            # ë””ë²„ê¹…ìš©: ë³¸ë¬¸ì„ ëª» ì°¾ì•˜ì„ ë•Œ HTMLì˜ ì¼ë¶€ë¥¼ í™•ì¸
            return title, None

    except Exception as e:
        return "ì—ëŸ¬", f"ì‹œìŠ¤í…œ ì—ëŸ¬: {e}"

# ==========================================
# 3. RSS íŒŒì‹± í•¨ìˆ˜ (ê°•ë ¥í•œ í—¤ë” ì¶”ê°€)
# ==========================================
def clean_html(raw_html):
    """CDATA íƒœê·¸ë‚˜ HTML íƒœê·¸ ì œê±°ìš© í—¬í¼ í•¨ìˆ˜"""
    if not raw_html:
        return ""
    # CDATA íƒœê·¸ ì œê±°
    clean = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', raw_html)
    # HTML íƒœê·¸ ì œê±° (<p>, <b> ë“±)
    clean = re.sub(r'<.*?>', '', clean)
    # íŠ¹ìˆ˜ë¬¸ì(&nbsp; ë“±) ì œê±°
    clean = re.sub(r'&[a-z]+;', '', clean)
    return clean.strip()

def get_latest_mofa_news():
    """
    ì™¸êµë¶€ ë¸”ë¡œê·¸ RSSë¥¼ ë’¤ì ¸ì„œ 'ì†Œì‹/ë³´ë„/ëŒ€ë³€ì¸' ê´€ë ¨ ê¸€ë§Œ ê°€ì ¸ì˜´
    """
    rss_url = "https://rss.blog.naver.com/mofakr.xml"
    
    # [ìˆ˜ì •] RSS ìš”ì²­ì—ë„ ê°•ë ¥í•œ í—¤ë” ì ìš© (ë„¤ì´ë²„ ì°¨ë‹¨ íšŒí”¼)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/xml,application/xhtml+xml,text/html;q=0.9, text/plain;q=0.8,image/png,*/*;q=0.5'
    }
    
    try:
        response = requests.get(rss_url, headers=headers)
        
        # íŒŒì‹±
        try:
            soup = BeautifulSoup(response.content, 'xml')
        except:
            soup = BeautifulSoup(response.content, 'html.parser')
            
        items = soup.find_all('item')
        target_links = []
        
        for item in items:
            category = item.category.text if item.category else ""
            title = item.title.text if item.title else ""
            link = item.link.text if item.link else ""
            
            # CDATA ë° ê³µë°± ì •ë¦¬
            title = clean_html(title)
            link = link.strip()
            
            if not link:
                continue

            # [í•„í„°ë§ ë¡œì§]
            if "ì†Œì‹" in category or "ë³´ë„" in category or "ëŒ€ë³€ì¸" in category or "ì™¸êµë¶€" in category:
                target_links.append({"title": title, "link": link})
                if len(target_links) >= 5: 
                    break
        
        # ë¹„ìƒìš©: íƒ€ê²Ÿ ê¸€ ì—†ìœ¼ë©´ ìµœì‹  3ê°œ ë¬´ì¡°ê±´ ê°€ì ¸ì˜¤ê¸°
        if not target_links and items:
            for i in items[:3]:
                t = clean_html(i.title.text)
                l = i.link.text.strip()
                if l:
                    target_links.append({"title": t, "link": l})
            
        return target_links

    except Exception as e:
        # RSS ì—°ê²° ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë¡œê·¸ëŠ” ì½˜ì†”ì—ë§Œ ì°ê³  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        print(f"RSS íŒŒì‹± ì—ëŸ¬: {e}")
        return []

# ==========================================
# 4. ìš”ì•½ í•¨ìˆ˜
# ==========================================
def predict_summary(text):
    if not text or len(text) < 50:
        return "ìš”ì•½í•  ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    input_ids = tokenizer.encode(text, return_tensors="pt", max_length=1024, truncation=True)

    summary_text_ids = model.generate(
        input_ids=input_ids,
        bos_token_id=model.config.bos_token_id,
        eos_token_id=model.config.eos_token_id,
        length_penalty=1.2,
        max_length=256,
        min_length=30,
        num_beams=4,
        early_stopping=True,
        no_repeat_ngram_size=3
    )
    
    summary = tokenizer.decode(summary_text_ids[0], skip_special_tokens=True)

    if summary and summary[-1] not in ['.', '!', '?']:
        last_punctuation = max(summary.rfind('.'), summary.rfind('!'), summary.rfind('?'))
        if last_punctuation != -1:
            summary = summary[:last_punctuation+1]

    return summary

# ==========================================
# 5. ë©”ì¸ UI í™”ë©´ êµ¬ì„±
# ==========================================
st.title("ğŸ“° ì™¸êµë¶€ ì†Œì‹ ìë™ ìš”ì•½ ë´‡")
st.write("ì¸ê³µì§€ëŠ¥ì´ ì™¸êµë¶€ ë¸”ë¡œê·¸ì˜ ì£¼ìš” ì†Œì‹ì„ 3ì¤„ë¡œ ìš”ì•½í•´ ë“œë¦½ë‹ˆë‹¤.")

if model is None:
    st.error("âš ï¸ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
else:
    st.success("AI ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ! (Ready)")

tab1, tab2 = st.tabs(["ğŸ”— URL ì§ì ‘ ì…ë ¥", "ğŸ“¢ ì™¸êµë¶€ ìµœì‹  ì†Œì‹ (ìë™)"])

# [Tab 1] URL ìš”ì•½
with tab1:
    st.subheader("ë‰´ìŠ¤/ë¸”ë¡œê·¸ ì£¼ì†Œ ì…ë ¥")
    input_url = st.text_input("ìš”ì•½í•˜ê³  ì‹¶ì€ ë„¤ì´ë²„ ë¸”ë¡œê·¸ URLì„ ì…ë ¥í•˜ì„¸ìš”:")
    
    if st.button("ìš”ì•½ ì‹œì‘", key="btn1"):
        if input_url:
            with st.spinner('í¬ë¡¤ë§ ë° ìš”ì•½ ì¤‘ì…ë‹ˆë‹¤...'):
                title, raw_text = get_naver_blog_content(input_url)
                
                if raw_text:
                    summary = predict_summary(raw_text)
                    st.markdown(f"### ğŸ“„ {title}")
                    st.info(summary)
                    with st.expander("ì›ë³¸ ë‚´ìš© ë³´ê¸°"):
                        st.write(raw_text)
                else:
                    st.error("ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì ‘ê·¼ ê¶Œí•œ í˜¹ì€ ì‚­ì œëœ ê¸€)")

# [Tab 2] ì™¸êµë¶€ ìµœì‹  ì†Œì‹
with tab2:
    st.subheader("ì™¸êµë¶€ ì£¼ìš” ì†Œì‹ (Top 5)")
    st.write("ì•„ë˜ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ 'ì™¸êµë¶€ ì†Œì‹/ë³´ë„' ì¹´í…Œê³ ë¦¬ì˜ ìµœì‹  ê¸€ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    
    if st.button("ìµœì‹  ì†Œì‹ ê°€ì ¸ì˜¤ê¸°", key="btn2"):
        with st.spinner('ì™¸êµë¶€ ë¸”ë¡œê·¸ë¥¼ ìŠ¤ìº”í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...'):
            news_items = get_latest_mofa_news()
            
            if not news_items:
                st.warning("ê°€ì ¸ì˜¬ ì†Œì‹ì´ ì—†ê±°ë‚˜ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            else:
                st.success(f"ì´ {len(news_items)}ê°œì˜ ìµœì‹  ì†Œì‹ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")
                
                for i, item in enumerate(news_items):
                    st.markdown("---")
                    st.markdown(f"**[{i+1}] {item['title']}**")
                    
                    _, content = get_naver_blog_content(item['link'])
                    
                    if content:
                        summary = predict_summary(content)
                        st.success(summary)
                    else:
                        st.caption("âš ï¸ ë³¸ë¬¸ í¬ë¡¤ë§ ì‹¤íŒ¨ (ë„¤ì´ë²„ ì°¨ë‹¨ ë˜ëŠ” ë¹„ê³µê°œ ê¸€)")
                        st.write(f"ë§í¬: {item['link']}")
