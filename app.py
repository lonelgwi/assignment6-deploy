import streamlit as st
import torch
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration
import requests
from bs4 import BeautifulSoup
import re
import trafilatura

# ==========================================
# 1. í˜ì´ì§€ ë° ëª¨ë¸ ì„¤ì •
# ==========================================
st.set_page_config(page_title="ì™¸êµë¶€ ì†Œì‹ ìš”ì•½ ë´‡", page_icon="ğŸ¤–")

@st.cache_resource
def load_model():
    try:
        model_name = "gogamza/kobart-summarization" 
        tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)
        model = BartForConditionalGeneration.from_pretrained(model_name)
        return tokenizer, model
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None, None

tokenizer, model = load_model()

# ==========================================
# 2. í¬ë¡¤ë§ í•¨ìˆ˜ (Trafilatura + íƒ€ì„ì•„ì›ƒ ê°•í™”)
# ==========================================
def get_naver_blog_content(url):
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„.
    ì‹¤íŒ¨ í™•ë¥ ì´ ë†’ìœ¼ë¯€ë¡œ ì§§ì€ íƒ€ì„ì•„ì›ƒì„ ë‘¡ë‹ˆë‹¤.
    """
    if not url: return "ì—ëŸ¬", None

    try:
        # ëª¨ë°”ì¼ ì£¼ì†Œë¡œ ë³€í™˜ (ì„±ê³µë¥ ì´ ì¡°ê¸ˆ ë” ë†’ìŒ)
        if "m.blog.naver.com" in url:
            target_url = url.replace("m.blog.naver.com", "blog.naver.com")
        else:
            target_url = url

        # 1ì°¨ ì‹œë„: Trafilatura
        downloaded = trafilatura.fetch_url(target_url)
        
        # 2ì°¨ ì‹œë„: PostView ì£¼ì†Œ ì§ì ‘ ì¡°ë¦½
        if downloaded is None:
            match = re.search(r'blog\.naver\.com/([a-zA-Z0-9_]+)/([0-9]+)', target_url)
            if match:
                final_url = f"https://blog.naver.com/PostView.naver?blogId={match.group(1)}&logNo={match.group(2)}"
                downloaded = trafilatura.fetch_url(final_url)

        if downloaded is None:
            return "ì°¨ë‹¨ë¨", None

        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        result_text = trafilatura.extract(downloaded, include_comments=False, include_tables=False, include_links=False)
        
        # ì œëª© ì¶”ì¶œ
        soup = BeautifulSoup(downloaded, 'html.parser')
        og_title = soup.select_one('meta[property="og:title"]')
        title = og_title['content'] if og_title else "ì œëª© ì—†ìŒ"

        if result_text:
            text = re.sub(r'\n+', ' ', result_text)
            return title, text.strip()
        else:
            return title, None

    except Exception:
        return "ì—ëŸ¬", None

# ==========================================
# 3. RSS íŒŒì‹± í•¨ìˆ˜ (í•µì‹¬: Descriptionê¹Œì§€ í™•ë³´)
# ==========================================
def clean_text(raw_html):
    """HTML íƒœê·¸ì™€ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°í•˜ì—¬ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ë‚¨ê¹€"""
    if not raw_html: return ""
    clean = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', raw_html) # CDATA ì œê±°
    clean = re.sub(r'<.*?>', '', clean) # HTML íƒœê·¸ ì œê±°
    clean = re.sub(r'&[a-z]+;', ' ', clean) # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    return clean.strip()

def get_latest_mofa_news():
    rss_url = "https://rss.blog.naver.com/mofakr.xml"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    try:
        response = requests.get(rss_url, headers=headers, timeout=5)
        soup = BeautifulSoup(response.content, 'html.parser') # xml íŒŒì„œ ëŒ€ì‹  html.parser ì‚¬ìš© (í˜¸í™˜ì„±)
        
        items = soup.find_all('item')
        target_list = []
        
        for item in items:
            category = item.category.text if item.category else ""
            title = clean_text(item.title.text if item.title else "")
            link = item.link.text.strip() if item.link else ""
            
            # [ë¹„ìƒìš©] RSSì— í¬í•¨ëœ ë³¸ë¬¸ ìš”ì•½ë³¸(Description) ê°€ì ¸ì˜¤ê¸°
            description = clean_text(item.description.text if item.description else "")

            if not link: continue

            # í•„í„°ë§
            if "ì†Œì‹" in category or "ë³´ë„" in category or "ëŒ€ë³€ì¸" in category or "ì™¸êµë¶€" in category:
                target_list.append({
                    "title": title, 
                    "link": link,
                    "desc": description  # ë¹„ìƒìš© ë³¸ë¬¸ ì €ì¥
                })
                if len(target_list) >= 5: break
        
        # í•„í„°ë§ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ìµœì‹  3ê°œë¼ë„ ê°€ì ¸ì˜´ (ë¹„ìƒìš©)
        if not target_list and items:
             for i in items[:3]:
                t = clean_text(i.title.text)
                l = i.link.text.strip()
                d = clean_text(i.description.text if i.description else "")
                target_list.append({"title": t, "link": l, "desc": d})

        return target_list

    except Exception as e:
        print(f"RSS ì—ëŸ¬: {e}")
        return []

# ==========================================
# 4. ìš”ì•½ í•¨ìˆ˜
# ==========================================
def predict_summary(text):
    if not text or len(text) < 20: # ê¸°ì¤€ ì™„í™”
        return "ìš”ì•½í•  ë‚´ìš©ì´ ë¶€ì¡±í•©ë‹ˆë‹¤."

    # ì…ë ¥ ë°ì´í„° ë³€í™˜
    input_ids = tokenizer.encode(text, return_tensors="pt", max_length=1024, truncation=True)

    # ìš”ì•½ë¬¸ ìƒì„±
    summary_text_ids = model.generate(
        input_ids=input_ids,
        bos_token_id=model.config.bos_token_id,
        eos_token_id=model.config.eos_token_id,
        length_penalty=1.0, # íŒ¨ë„í‹° ì™„í™”
        max_length=128,
        min_length=20,      # ìµœì†Œ ê¸¸ì´ ì™„í™”
        num_beams=4,
        early_stopping=True
    )
    
    summary = tokenizer.decode(summary_text_ids[0], skip_special_tokens=True)
    return summary

# ==========================================
# 5. ë©”ì¸ UI
# ==========================================
st.title("ğŸ“° ì™¸êµë¶€ ì†Œì‹ ìë™ ìš”ì•½ ë´‡")
st.write("ì¸ê³µì§€ëŠ¥ì´ ì™¸êµë¶€ ë¸”ë¡œê·¸ì˜ ì£¼ìš” ì†Œì‹ì„ 3ì¤„ë¡œ ìš”ì•½í•´ ë“œë¦½ë‹ˆë‹¤.")

if model is None:
    st.error("âš ï¸ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨.")
else:
    st.success("AI ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ! (Ready)")

tab1, tab2 = st.tabs(["ğŸ”— URL ì§ì ‘ ì…ë ¥", "ğŸ“¢ ì™¸êµë¶€ ìµœì‹  ì†Œì‹ (ìë™)"])

with tab1:
    st.subheader("ë‰´ìŠ¤/ë¸”ë¡œê·¸ ì£¼ì†Œ ì…ë ¥")
    input_url = st.text_input("URL ì…ë ¥:")
    if st.button("ìš”ì•½ ì‹œì‘", key="btn1"):
        if input_url:
            with st.spinner('ë¶„ì„ ì¤‘...'):
                title, raw_text = get_naver_blog_content(input_url)
                if raw_text:
                    st.markdown(f"### ğŸ“„ {title}")
                    st.info(predict_summary(raw_text))
                    with st.expander("ì›ë³¸ ë³´ê¸°"): st.write(raw_text)
                else:
                    st.error("ë³¸ë¬¸ ì ‘ê·¼ì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")

with tab2:
    st.subheader("ì™¸êµë¶€ ì£¼ìš” ì†Œì‹ (Top 5)")
    st.write("ë³¸ë¬¸ ì ‘ì†ì´ ì°¨ë‹¨ë  ê²½ìš°, ë„¤ì´ë²„ê°€ ì œê³µí•œ ë¯¸ë¦¬ë³´ê¸° ë‚´ìš©ì„ ëŒ€ì‹  ìš”ì•½í•©ë‹ˆë‹¤.")
    
    if st.button("ìµœì‹  ì†Œì‹ ê°€ì ¸ì˜¤ê¸°", key="btn2"):
        with st.spinner('ì†Œì‹ ê°€ì ¸ì˜¤ëŠ” ì¤‘...'):
            news_items = get_latest_mofa_news()
            
            if not news_items:
                st.warning("RSS ì—°ê²° ì‹¤íŒ¨.")
            else:
                st.success(f"ì´ {len(news_items)}ê°œì˜ ì†Œì‹ í™•ì¸")
                
                for i, item in enumerate(news_items):
                    st.markdown("---")
                    st.markdown(f"**[{i+1}] {item['title']}**")
                    
                    # 1. í¬ë¡¤ë§ ì‹œë„
                    _, content = get_naver_blog_content(item['link'])
                    
                    if content:
                        # ì„±ê³µ ì‹œ ë³¸ë¬¸ ìš”ì•½
                        st.success(predict_summary(content))
                    elif item['desc']:
                        # [ë¹„ìƒìš©] ì‹¤íŒ¨ ì‹œ RSS Description ìš”ì•½
                        st.warning("ğŸ”’ ë³¸ë¬¸ ì ‘ì† ì°¨ë‹¨ë¨ â†’ ë¯¸ë¦¬ë³´ê¸° ë‚´ìš©ìœ¼ë¡œ ëŒ€ì²´ ìš”ì•½í•©ë‹ˆë‹¤.")
                        st.info(predict_summary(item['desc']))
                    else:
                        st.error("ìš”ì•½í•  ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
                    st.caption(f"[ì›ë¬¸ ë³´ëŸ¬ê°€ê¸°]({item['link']})")
