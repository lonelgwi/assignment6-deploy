import streamlit as st
import torch
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration
import requests
from bs4 import BeautifulSoup
import re

# ==========================================
# 1. í˜ì´ì§€ ë° ëª¨ë¸ ì„¤ì •
# ==========================================
st.set_page_config(page_title="ì™¸êµë¶€ ì†Œì‹ ìš”ì•½ ë´‡", page_icon="ğŸ¤–")

@st.cache_resource
def load_model():
    try:
        # ê¹ƒí—ˆë¸Œ ìš©ëŸ‰ ì œí•œ ì—†ì´ ì‹¤í–‰ë˜ë„ë¡ ê³µê°œëœ KoBART ëª¨ë¸ ì‚¬ìš©
        # (ì œê³µí•´ì£¼ì‹  ì½”ë“œì˜ ë¡œì§ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜, ëª¨ë¸ íŒŒì¼ë§Œ ì˜¨ë¼ì¸ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤)
        model_name = "gogamza/kobart-summarization" 
        tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)
        model = BartForConditionalGeneration.from_pretrained(model_name)
        return tokenizer, model
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None, None

tokenizer, model = load_model()

# ==========================================
# 2. í¬ë¡¤ë§ í•¨ìˆ˜ (ì œê³µí•´ì£¼ì‹  ë¡œì§ ì ìš©)
# ==========================================
def get_naver_blog_content(url):
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ URL -> ì œëª©, ë³¸ë¬¸ ì¶”ì¶œ (Iframe êµ¬ì¡° ëŒ€ì‘)
    """
    headers = {'User-Agent': 'Mozilla/5.0'}
    try:
        # ëª¨ë°”ì¼ ì£¼ì†Œ ë³€í™˜
        if "m.blog.naver.com" in url:
            url = url.replace("m.blog.naver.com", "blog.naver.com")

        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')

        # Iframe ëŒ€ì‘
        iframe = soup.select_one('iframe#mainFrame')
        if iframe:
            real_url = "https://blog.naver.com" + iframe['src']
            response = requests.get(real_url, headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')

        # ì œëª© ì¶”ì¶œ (.se-title-text ë˜ëŠ” .htitle)
        title_elem = soup.select_one('.se-title-text') or soup.select_one('.htitle')
        title = title_elem.text.strip() if title_elem else "ì œëª© ì—†ìŒ"

        # ë³¸ë¬¸ ì¶”ì¶œ (.se-main-container ë˜ëŠ” #postViewArea)
        content_elem = soup.select_one('.se-main-container') or soup.select_one('#postViewArea')

        if content_elem:
            text = content_elem.text
            # ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ ë° ê³µë°± ì •ë¦¬
            text = re.sub(r'\n+', ' ', text)
            text = re.sub(r'\s+', ' ', text)
            return title, text.strip()
        else:
            return title, None

    except Exception as e:
        return "ì—ëŸ¬", f"í¬ë¡¤ë§ ì—ëŸ¬: {e}"

# ==========================================
# 3. RSS íŒŒì‹± í•¨ìˆ˜ (ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ì¶”ê°€)
# ==========================================
def get_latest_mofa_news():
    """
    ì™¸êµë¶€ ë¸”ë¡œê·¸ RSSë¥¼ ë’¤ì ¸ì„œ 'ì†Œì‹/ë³´ë„/ëŒ€ë³€ì¸' ê´€ë ¨ ê¸€ë§Œ ê°€ì ¸ì˜´
    """
    rss_url = "https://rss.blog.naver.com/mofakr.xml"
    
    try:
        response = requests.get(rss_url)
        # lxmlì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ xml íŒŒì‹± ì‹œë„
        try:
            soup = BeautifulSoup(response.content, 'xml')
        except:
            soup = BeautifulSoup(response.content, 'html.parser')
            
        items = soup.find_all('item')
        
        target_links = []
        
        for item in items:
            # ì¹´í…Œê³ ë¦¬ íƒœê·¸ í™•ì¸
            category = item.category.text if item.category else ""
            title = item.title.text
            link = item.link.text
            
            # [í•„í„°ë§ ë¡œì§] ì‚¬ìš©ìê°€ ì›í•œ 'ì™¸êµë¶€ ì†Œì‹' ê´€ë ¨ í‚¤ì›Œë“œ
            if "ì†Œì‹" in category or "ë³´ë„" in category or "ëŒ€ë³€ì¸" in category or "ì™¸êµë¶€" in category:
                target_links.append({"title": title, "link": link})
                
                # ìµœì‹  5ê°œë§Œ ìˆ˜ì§‘í•˜ë©´ ì¤‘ë‹¨
                if len(target_links) >= 5: 
                    break
        
        # ë§Œì•½ íƒ€ê²Ÿ ì¹´í…Œê³ ë¦¬ ê¸€ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ìµœì‹ ê¸€ 3ê°œë¼ë„ ê°€ì ¸ì˜¤ê¸° (ë¹„ìƒìš©)
        if not target_links and items:
            target_links = [{"title": i.title.text, "link": i.link.text} for i in items[:3]]
            
        return target_links

    except Exception as e:
        st.error(f"RSS íŒŒì‹± ì‹¤íŒ¨: {e}")
        return []

# ==========================================
# 4. ìš”ì•½ í•¨ìˆ˜ (í›„ì²˜ë¦¬ ë¡œì§ ì ìš©)
# ==========================================
def predict_summary(text):
    if not text or len(text) < 50:
        return "ìš”ì•½í•  ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    # ì…ë ¥ ê¸¸ì´ ì œí•œ (Truncation)
    input_ids = tokenizer.encode(text, return_tensors="pt", max_length=1024, truncation=True)

    # ëª¨ë¸ ìƒì„± ì˜µì…˜ (ì œê³µí•´ì£¼ì‹  íŒŒë¼ë¯¸í„° ì ìš©)
    summary_text_ids = model.generate(
        input_ids=input_ids,
        bos_token_id=model.config.bos_token_id,
        eos_token_id=model.config.eos_token_id,
        length_penalty=1.2,   # ìì—°ìŠ¤ëŸ¬ìš´ ê¸¸ì´ ìœ ë„
        max_length=256,       # ê¸¸ì´ í™•ì¥
        min_length=30,
        num_beams=4,
        early_stopping=True,
        no_repeat_ngram_size=3
    )
    
    summary = tokenizer.decode(summary_text_ids[0], skip_special_tokens=True)

    # [í›„ì²˜ë¦¬] ë¬¸ì¥ ëŠê¹€ ë°©ì§€
    if summary and summary[-1] not in ['.', '!', '?']:
        last_punctuation = max(summary.rfind('.'), summary.rfind('!'), summary.rfind('?'))
        if last_punctuation != -1:
            summary = summary[:last_punctuation+1]

    return summary

# ==========================================
# 5. ë©”ì¸ UI í™”ë©´ êµ¬ì„±
# ==========================================

st.title("ğŸ“° ì™¸êµë¶€ ì†Œì‹ ìë™ ìš”ì•½ ë´‡")
st.write("ì¸ê³µì§€ëŠ¥ì´ ì™¸êµë¶€ ë¸”ë¡œê·¸ì˜ ì£¼ìš” ì†Œì‹ì„ 3ì¤„ë¡œ ìš”ì•½í•´ ë“œë¦½ë‹ˆë‹¤.")

if model is None:
    st.error("âš ï¸ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
else:
    st.success("AI ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ! (Ready)")

# íƒ­ êµ¬ì„±
tab1, tab2 = st.tabs(["ğŸ”— URL ì§ì ‘ ì…ë ¥", "ğŸ“¢ ì™¸êµë¶€ ìµœì‹  ì†Œì‹ (ìë™)"])

# [Tab 1] URL ìš”ì•½
with tab1:
    st.subheader("ë‰´ìŠ¤/ë¸”ë¡œê·¸ ì£¼ì†Œ ì…ë ¥")
    input_url = st.text_input("ìš”ì•½í•˜ê³  ì‹¶ì€ ë„¤ì´ë²„ ë¸”ë¡œê·¸ URLì„ ì…ë ¥í•˜ì„¸ìš”:")
    
    if st.button("ìš”ì•½ ì‹œì‘", key="btn1"):
        if input_url:
            with st.spinner('í¬ë¡¤ë§ ë° ìš”ì•½ ì¤‘ì…ë‹ˆë‹¤...'):
                title, raw_text = get_naver_blog_content(input_url)
                
                if raw_text:
                    summary = predict_summary(raw_text)
                    st.markdown(f"### ğŸ“„ {title}")
                    st.info(summary)
                    with st.expander("ì›ë³¸ ë‚´ìš© ë³´ê¸°"):
                        st.write(raw_text)
                else:
                    st.error("ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì ‘ê·¼ ê¶Œí•œ í˜¹ì€ ì‚­ì œëœ ê¸€)")

# [Tab 2] ì™¸êµë¶€ ìµœì‹  ì†Œì‹ (ìë™ ìˆ˜ì§‘)
with tab2:
    st.subheader("ì™¸êµë¶€ ì£¼ìš” ì†Œì‹ (Top 5)")
    st.write("ì•„ë˜ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ 'ì™¸êµë¶€ ì†Œì‹/ë³´ë„' ì¹´í…Œê³ ë¦¬ì˜ ìµœì‹  ê¸€ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    
    if st.button("ìµœì‹  ì†Œì‹ ê°€ì ¸ì˜¤ê¸°", key="btn2"):
        with st.spinner('ì™¸êµë¶€ ë¸”ë¡œê·¸ë¥¼ ìŠ¤ìº”í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...'):
            # 1. RSS ë¦¬ìŠ¤íŠ¸ í™•ë³´
            news_items = get_latest_mofa_news()
            
            if not news_items:
                st.warning("ê°€ì ¸ì˜¬ ì†Œì‹ì´ ì—†ê±°ë‚˜ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            else:
                st.success(f"ì´ {len(news_items)}ê°œì˜ ìµœì‹  ì†Œì‹ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")
                
                # 2. ê° ê²Œì‹œê¸€ ìˆœíšŒí•˜ë©° í¬ë¡¤ë§ & ìš”ì•½
                for i, item in enumerate(news_items):
                    st.markdown("---")
                    st.markdown(f"**[{i+1}] {item['title']}**")
                    
                    # ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§
                    _, content = get_naver_blog_content(item['link'])
                    
                    if content:
                        # ìš”ì•½ ì‹¤í–‰
                        summary = predict_summary(content)
                        st.success(summary)
                    else:
                        st.caption("ë³¸ë¬¸ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
